<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Data Science 101: Precision vs. recall | Mel Hanna</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Data Science 101: Precision vs. recall" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Understanding the intuition behind these performance metrics." />
<meta property="og:description" content="Understanding the intuition behind these performance metrics." />
<link rel="canonical" href="https://mdhanna.github.io/blog/precision-vs-recall/" />
<meta property="og:url" content="https://mdhanna.github.io/blog/precision-vs-recall/" />
<meta property="og:site_name" content="Mel Hanna" />
<meta property="og:image" content="https://mdhanna.github.io/blog/images/precision_recall/Capture-5.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-31T18:47:51-05:00" />
<script type="application/ld+json">
{"url":"https://mdhanna.github.io/blog/precision-vs-recall/","@type":"BlogPosting","headline":"Data Science 101: Precision vs. recall","dateModified":"2020-05-31T18:47:51-05:00","datePublished":"2020-05-31T18:47:51-05:00","image":"https://mdhanna.github.io/blog/images/precision_recall/Capture-5.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://mdhanna.github.io/blog/precision-vs-recall/"},"description":"Understanding the intuition behind these performance metrics.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://mdhanna.github.io/blog/feed.xml" title="Mel Hanna" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','G-36543P9G91','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Mel Hanna</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Data Science 101: Precision vs. recall</h1><p class="page-description">Understanding the intuition behind these performance metrics.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-05-31T18:47:51-05:00" itemprop="datePublished">
        May 31, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#Data Science Basics">Data Science Basics</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Congrats!  You’ve built a binary classifier that you’re convinced is totally awesome.</p>

<p>But how do you quantify just how awesome this model is?  And more specifically, how do you communicate this model’s level of awesomeness to your manager/product owner/stakeholder/random-person-on-the-street?</p>

<p>This is where performance metrics such as precision and recall come into play, and we’ll attempt to explain the intuition around these in addition to their definitions.</p>

<h2 id="what-is-the-positive-class">What is the positive class?</h2>

<p>If you’ve built a binary classifier, perhaps the first step in determining your performance metric is to select a “positive” class.  This is easy in some instances (ex. coronavirus test result) and less so in others (ex. determining if your pet is either a cat or a chinchilla).  But establishing these definitions early (and stating them explicitly) will save a lot of confusion down the road.</p>

<h2 id="false-negatives-vs-true-positives">False negatives vs. true positives</h2>

<p>Something I struggled with initially was what exactly did we mean by a “false negative”?  Was it a <em>true</em> negative that we classified incorrectly?  Or did the model return an incorrect (and therefore false) <em>prediction</em> of negative?  In other words, from whose perspective do we consider this classification false?</p>

<p>The answer is the latter definition above.  The table below sums this up.</p>

<p><img src="/blog/images/precision_recall/Capture-5.png" alt="" /></p>

<h2 id="confusion-matrix">Confusion matrix</h2>

<p>Once you’ve established your false positives and false negatives, you can display them in a confusion matrix that looks very similar to the table above.</p>

<p>Here is an example for a classifier that attempts to determine if shoplifting is taking place (where we define a shoplifting incident as a <em>positive</em>).  Let’s say for example, this model takes video footage from a store surveillance system and looks for certain features (ex. a customer picking up an item and hiding it under his/her shirt) that would indicate shoplifting.</p>

<p>The confusion matrix shows the number of observations for each class and the corresponding predictions from the model.
<img src="/blog/images/precision_recall/download-5.png" alt="" /></p>

<p>From the matrix above, we can see that our classifier is rather paranoid and often mistakes normal behavior for shoplifting.</p>

<h2 id="accuracy">Accuracy</h2>

<p>Now we can start to sum up the classifier’s performance using a single value, such as the <strong>accuracy</strong>, which represents the fraction of correct predictions out of the total.  In the shoplifting example, the accuracy is shown by the following:</p>

<p><br /></p>
<p style="text-align: center;">$accuracy = \frac{\text{ true positives}\ + \text{ true negatives}}{\text{ total classified}} = \frac{40+ 107}{40+ 107+ 345 + 8} = 0.294$</p>
<p><br />
Unfortunately, this model is performing far worse than random.</p>

<p><strong>WARNING:</strong> Note that accuracy is a <strong>misleading metric</strong> in this case due to unbalanced class sizes.  In other words,  because we have so few true shoplifting incidences compared to cases of normal behavior, we can easily achieve an accuracy of 0.904 by returning a prediction of “normal” every time.  But no one would consider such a classifier to be truly “accurate”.</p>

<h2 id="precision-and-recall">Precision and Recall</h2>

<p>I mentioned earlier that our classifier tends to overpredict shoplifting–how can we incorporate this tendency into a performance metric?</p>

<p>This is where precision and recall come into play.  These metrics are <strong>class-specific</strong>, which means that we must specify a value for both precision and recall for each class returned by the model.</p>

<h4 id="precision"><strong>Precision</strong></h4>

<p>Precision is the answer to the question: out of the <strong>total predictions</strong> for a certain class returned by the model, how many were actually correct?  For example, the precision for the shoplifting class is:</p>

<p><br /></p>
<p style="text-align: center;">$precision = \frac{TP}{TP\ +\ FP}$</p>
<p><br /></p>
<p style="text-align: center;">$precision_{shoplifting} = \frac{\text{ true shoplifting}}{\text{ total predicted shoplifting}} = \frac{40}{40+345} = 0.104$</p>
<p><br /></p>

<p>Similarly, for the normal behavior:</p>

<p><br /></p>
<p style="text-align: center;">$precision_{normal} = \frac{\text{ true normal}}{\text{ total predicted normal}} = \frac{107}{107+8} = 0.930$</p>
<p><br /></p>

<p>In other words, the model’s predictions for normal behavior were correct 93% of the time, while its predictions for shoplifting were correct only <em>10%</em> of the time.  Yikes.</p>

<h4 id="recall"><strong>Recall</strong></h4>

<p>I like to think of recall as a <strong>class-specific accuracy</strong>.  How many of the model’s predictions for a certain class were actually correct?
<br /></p>
<p style="text-align: center;">$recall = \frac{TP}{TP\ +\ FN}$</p>
<p><br /></p>
<p style="text-align: center;">$recall_{shoplifting} = \frac{\text{ true shoplifting}}{\text{ total actual shoplifting}} = \frac{40}{40+8} = 0.833$</p>
<p><br /></p>
<p style="text-align: center;">$recall_{normal} = \frac{\text{ true normal}}{\text{ total actual normal}} = \frac{107}{107+345} = 0.237$</p>
<p><br /></p>

<p>The model correctly identified 83% of the actual shoplifting incidents.</p>

<p>And here we see the trade-off often inherent in precision and recall.  The model correctly predicted a good majority (83%) of the actual shoplifting incidents, but at the expense of also erroneously predicting many truly normal behaviors as shoplifting too (nearly 90% of the predicted shoplifting incidents).</p>

<p>This ties into <strong>Type I and Type II</strong> errors, where a type I error is a false positive (normal behavior incorrectly classified as shoplifting) and a type II error is a false negative (shoplifting incorrectly classified as normal behavior).</p>

<p>For our situation, this boils down to asking if you would rather have the police called on an innocent customer (type I) or lose merchandise to unchecked shoplifters (type II)?<br />
Understanding the costs of type I and type II errors helps to weigh whether you’d like to improve either recall or precision.  Often, it’s difficult to do both simultaneously.</p>

<h2 id="f1-score">F1-Score</h2>

<p>However, if you’d like to tie up both precision and recall into one single metric to hand over to your manager/product owner/stakeholder, then the F1-score (sometimes called F-measure) is for you!</p>

<p>This is simply the harmonic mean of the precision and recall for a given class, shown below.</p>

<p><br /></p>
<p style="text-align: center;">$F1 = 2 * \frac{precision\ *\ recall}{precision\ +\ recall}$</p>
<p><br /></p>

<p>An F1-score of 1 indicates perfect precision and recall.</p>

<p>If you’d like to place more importance on recall over precision, you can introduce a $\beta$ term (set $\beta$ to a value less than 1 to place more emphasis of precision instead of recall).</p>

<p><br /></p>
<p style="text-align: center;">$F_{\beta} = (1\ +\ \beta^2) * \frac{precision\ *\ recall}{(\beta^2\ *\ precision)\ +\ recall}$</p>
<p><br /></p>


  </div><a class="u-url" href="/blog/precision-vs-recall/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Random thoughts and fun projects.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/mdhanna" title="mdhanna"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
