{
  
    
  
    
        "post1": {
            "title": "Being Radically Candid amidst the Chaos of 2020",
            "content": "It’d be an understatement to say that so far, 2020 has been a tough year for nearly everyone. From a global pandemic sickening millions of people to civil unrest rocking the United States and beyond, the world seems to have turned upside down. . If you’ve found yourself in a management position during this chaos, you may be wondering how best to navigate the shift of your company to remote work, the mental health of your team, and the need to address systemic racism in your organization. . Applying the concepts behind “Radical Candor” can help you tackle these issues head-on. And anyone—from CEO’s to individual team members—can start using these lessons today to begin effecting change. . What is Radical Candor? . Radical Candor is a 2017 book and management philosophy from Kim Scott, a former manager at Apple and Google. We can sum it up with the following: . Managers should care personally and challenge directly. . Fleshing this out a bit more, Scott created a matrix to show how managers might fall short on either of these goals. . Credit to radicalcandor.com Obnoxious Aggression: A manager who isn’t afraid to challenge his/her employees but has made no effort to show that he/she cares about them as people or is invested in their success. . | Ruinous Empathy: A manager who shies away from providing “uncomfortable” criticism out of fear he/she may hurt their employee’s feelings. The vast majority of managers fall into this category, and this is definitely where I naturally land. . | Manipulative Insincerity: A manager who doesn’t bother to give any direct feedback or show interest in his/her employees’ careers. In other words, the worst kind of manager you could get. . | Radical Candor: A manager who recognizes that giving direct feedback in a respectful manner is the best way to help his/her employees succeed and who takes the time to demonstrate personal and professional investment in them. . | . To sum up, promoting a trusting environment where team members aren’t afraid to challenge each other or their manager is the quickest way to organizational success. I can’t imagine how much time and productivity we lose by withholding from someone the feedback they desperately need if they want to improve their performance at work (or anywhere!). . Compound Interest of Continuous Feedback . Scott also emphasizes the importance of real-time feedback. You might typically bottle up all your feedback for Employee Eric throughout the week and then unleash it on him during your regularly scheduled one-on-one. This can backfire for two reasons. . Sense of Whiplash: The situation for which you’re giving Eric this feedback—maybe he presented a sloppy demo to marketing on Monday and couldn’t answer any follow-up questions from the team—is now far in the past. Eric might have thought he knocked that presentation out of the park, and he internalized that view for several days before you dashed cold water all over it. . | Repeat Offender: Even worse, Eric might have already given another presentation in the meantime with the same poor quality and lackluster results. . | . While these two possibilities should be reason enough to start giving feedback immediately whenever possible, I like to think about real-time feedback as analogous to compound interest. Any armchair investor knows that continuously compounded interest grows at a much faster rate than annual or “simple” interest. . Credit to fool.com Feedback works the same way. If we frequently give small amounts of both positive and negative feedback, the recipient will compound their growth accordingly. . Radical Candor Today . Ok, this all sounds like a great way to run a company in normal times. But these are not normal times. What lessons can we learn for today? . Caring Personally . In typical circumstances, showing that you care personally about your employees or team members might not be that simple. Some people don’t like to discuss their personal lives at work or make small talk, especially introverts. And building trust organically takes time. . But today, checking in on the personal lives of your co-workers and especially your direct reports isn’t just sanctioned—it’s expected. . When we first moved to WFH, we needed to understand how this sudden shift was affecting those around us in the new virtual workplace. . Are they feeling isolated/burnt-out/unmotivated? . | Do they have kids home from school which affect what hours they can be online? . | Are they caring for elderly relatives or neighbors that might add to whatever stress they’re already feeling? . | . These conversations started to crack open the door to discuss feelings and invite vulnerability as the line between personal and professional started to blur. . The riots incited by the death of George Floyd and his death itself also warranted checking in with our colleagues. . How are they coping with the sense of unrest roiling our country? . | How are they feeling in general given current events? . | Has their neighborhood been looted or burned? . | Are they safe? . | . These last questions, especially, are not ones I ever expected to ask in my role as a manager. And while I hope these circumstances will never be repeated, I am grateful that this situation has destigmatized discussing our personal emotions at work and has given me the opportunity to show that I care personally about my team as human beings. . Confronting Racism . Our current crisis also necessitates that we act along the other axis above—challenging directly. . I admit that I fell into the contingent of white people who put our heads in the sand by believing that by simply being “not racist”, we had overcome ingrained biases and systemic prejudice in this country. . The death of George Floyd and the protests sweeping the country were a long overdue wake-up call, and like many of my peers, I took the time to try and educate myself. I’ve been reading “How to Be an Anti-Racist” by Ibram X. Kendi, which has been eye-opening. I’m ashamed that I wasn’t aware of much of the historical context around concepts of race and hadn’t realized how claiming to be “colorblind” actually hurt communities of color by turning a blind eye to racist policies. . Kendi’s proposed antidote is that we become actively anti-racist. We must constantly evaluate our beliefs, actions, and words for unconscious bias. Yes, that sounds exhausting, and it is. But people of color have been exhausted for centuries—from slavery, from blatant discrimination, from the possibility of being shot by the police—so much so that it has taken a toll on their physical and mental health. . And we must adopt this anti-racist attitude in the workplace, as well, by challenging directly. Kim Scott herself provided an example of unconscious racism embedded in the first edition of her book in a recent blog post. The book suggested using a stuffed monkey called “Whoops the Monkey” as a prop in the office to encourage team members to discuss their mistakes. As a white woman, Scott did not realize that being called a monkey is a common denigration targeted at black people, and bringing this symbol into the office, especially as a representation of mistakes, was inappropriate. . Likely she would never have known had not someone spoken up. Only by directly challenging those we see engaging in acts of racism, even unconsciously, can we start to affect real change in mindsets, language, and policies. . Conclusion . Radical Candor offers an actionable framework for managers (or anyone!) to create an open environment where direct feedback is delivered promptly and where empathy establishes a relationship of trust. These aspirations also have immediate applications to the world of 2020—by engaging with our co-workers’ personal needs amidst continuing stress and by directly confronting racial attitudes and policies in the workplace. .",
            "url": "https://mdhanna.github.io/blog/management/2020/06/29/being-radically-candid-amidst-the-chaos-of-2020.html",
            "relUrl": "/management/2020/06/29/being-radically-candid-amidst-the-chaos-of-2020.html",
            "date": " • Jun 29, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Applying DAG's to Causal Models",
            "content": "I’ve been reading “The Book of Why” by Judea Pearl over the past few weeks, which has really helped formalize my intuition of causation. However, the book would be much better if Pearl left out any sentences written in the first-person as he has an annoying tendency to style himself as a messiah proclaiming the enlightened concepts of Causation to all the lowly statisticians still stuck on Correlation. . If we can look past his self-aggrandizing remarks, “The Book of Why” applies causal models to examples from the surgeon general’s committee on smoking in the 1960’s to the Monty Hall paradox. By reducing these multi-faceted problems down to a causal representation, we can finally put our finger on contributing factors or “causes” and control for them (if possible) to isolate the effect we are attempting to discover. . Perhaps the biggest takeaway for me from this book is the need to understand the data generation process when working with a dataset. This might sound like a no-brainer but too often, data scientists are so eager to jump in to the big shiny ball pit of a new dataset that they don’t stop to think about what this data actually represents. . Data scientist with a new dataset By including the process by which the data was generated in these causal models, we can augment our own mental model and unlock the true relationships behind the variables of interest. . So what’s a DAG? . Directly acyclic graphs (DAG’s) are a visual representation of a causal model. Here’s a simple one: . . You were late for work because you had to change your car’s tire because it was flat. Of course, we could add on much more than this (why was it flat?) but you get the idea. . Junction Types . Let’s explore what we can do with DAG’s through different junction types. . Chain . This is the simplest DAG and is represented in the example above. A generalized representation below shows that A is a cause of B, which is itself a cause of C. . . Collider . Now we have two causes for C. Both A and B affect the outcome C. . . Conditioning on C will reveal a non-causal, negative correlation between A &amp; B. This correlation is called collider bias. . We can understand this effect in crude mathematical terms. If A + B = C and we hold C constant, then we must increase A by the same amount we decrease B. . Additionally, this phenomenon is sometimes also called the “explain-away effect” because C “explains away” the correlation between A and B. . Note that the collider bias may be positive in cases when contributions from both A and B are necessary to affect C. . An example of a collider relationship would be the age-old nature vs. nurture question. Someone’s personality (C) is a product of both their upbringing (A) and the genes (B). . Fork . In the case of a fork, A affects both B and C. . . Without conditioning on A, there exists a spurious (non-causal) correlation between B &amp; C. A classic example of a spurious correlation is the relationship between crime (B) and ice cream sales (C). When you plot these two values over time, they appear to increase and decrease together, suggesting some kind of causality. Does ice cream cause people to commit crime? . Of course, this relationship can be explained by adding in temperature (A). Warmer weather causes people to leave their homes more often, leading to more crime (B). People also crave ice cream cones (C) on hot days. . Node Types . Mediators . A mediator is the node that “mediates” or transmits a causal effect from one node to another. . Again using the example below, B mediates the causal effect of A onto C. . . Confounders . Harking back to the crime-and-ice-cream example, temperature is the confounder node as it “confounds” the relationship between ice cream sales and crime. . . If we control for the confounder (A), we can isolate the relationship between C and B, if one exists. This is a key concept for experimental design. . Correcting for Confounding . Let’s spend some more time on this subject. Pearl’s assertion is that if we control for all confounders, we should be able to isolate the relationship between the variables of interest and therefore prove causation, instead of mere correlation. . Pearl defines confounding more broadly as any relationship that leads to $P(Y vert do(X)) neq P(Y vert X)$, where the $do$ operator implies an action. In other words, if there is a difference between the probability of an outcome $Y$ given $X$ and the probability of $Y$ given $X$ in a perfect world in which we were able to change $X$ and only $X$, then confounding is afoot. . Four Rules of Information Flow . Pearl has 4 rules for controlling the flow of information through a DAG. . In a chain (A → B → C), B carries information from A to C. Therefore, controlling for B prevents information about A from reaching C and vice versa. . | In a fork (A ← B → C), B is the only known common source of information between both A and C. Therefore, controlling for B prevents information about A from reaching C and vice versa. . | In a collider (A → B ← C), controlling for B “opens up” the pipe between A and C due to the explain-away effect. . | Controlling for descendants of a variable will partially control for the variable itself. Therefore, controlling the descendant of a mediator partially closes the pipe, and controlling for the descendant of a collider partially opens the pipe. . | Back-door criterion . We can use these causal models as represented by DAG’s to determine how exactly we should remove this confounding from our study. . If we are interested in understanding the relationship between only X and Y, we must identify and dispatch any confounding back-door paths, where a back-door path is any path from X to Y that starts with an arrow into X. . Pearl’s Games . Pearl devises a series of games that involve increasingly complicated DAG’s where the objective is to “deconfound” the path from X to Y. This is achieved by blocking every non-causal path while leaving all causal paths intact. . In other words, we need to identify and block all back-door paths while ensuring that any variable Z on a back-door path is not a descendant of X via a causal path to Y. . Let’s go through some examples, using the numbered games from the book. . Game 2 . . We need to determine which variables (if any) of A, B, C, D, or E need to be controlled in order to deconfound the path from X to Y. . There is one back-door path: X ← A → B ← D → E → Y. This path is blocked by the collider at B from the third rule of information flow. . Therefore, there is no need to control any of these variables! . Game 5 . . This one’s a bit more interesting. We have two back-door paths: . X ← A → B ← C → Y | X ← B ← C → Y | The first back-door path is blocked by a collider at B so there is no need to control any variables due to this relationship. . The second path, however, represents a non-causal path between X and Y. We need to control for either B or C. . But watch out! If we control for B, we fall into the condition outlined by Pearl’s third rule above, where we’ve controlled for a collider and thus opened up the first back-door path in this diagram. . Therefore, if we control for B, we will then have to control for A or C as well. However, we can also control for only C initially and avoid the collider bias altogether. . Conclusion . DAG’s can be an informative way to organize our mental models around causal relationships. Keeping in mind Pearl’s Four Rules of Information Flow, we can identify confounding variables that cloud the true relationship between the variables under study. . Bringing this home for data scientists, when we include the data generation process as a variable in a DAG, we remove much of the mystery surrounding such pitfalls as Simpson’s Paradox. We’re able to think more like informed humans and less like data-crunching machines—an ability we should all be striving for in our increasingly AI-driven world. .",
            "url": "https://mdhanna.github.io/blog/things%20i'm%20reading/2020/06/24/applying-dags-to-causal-models.html",
            "relUrl": "/things%20i'm%20reading/2020/06/24/applying-dags-to-causal-models.html",
            "date": " • Jun 24, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Data Science 101: The ROC curve",
            "content": "Precision and recall aren’t the only ways to quantify our performance when developing a classifier. . Plotting a Receiver Operating Characteristic (ROC) curve is another useful tool to help us quickly determine how well the classifier performs and visualize any trade-offs we might be making as we attempt to balance Type I and Type II error rates. . The basics . Let’s jump right in. Here’s an ROC curve for a model that predicts credit card default (where a positive is considered to be a default). . . Axes . The x-axis represents the False Positive Rate (FPR) or the probability of a false alarm. This can be calculated through: . . $FPR = frac{false positives}{false positives + true negatives}$ . . Put another way, FPR represents the fraction of incorrectly classified negatives (in this case, accounts that did not default but for which our model predicted would default) within the total population of negatives (all accounts that did not default). . The y-axis shows the True Positive Rate (TPR), which is equivalent to the recall. . Recall is just class-specific accuracy or: . . $recall = frac{true positives}{true positives + false negatives} $ . . Again, in the context of our example, this is the fraction of accounts that did default and for which our model predicted they would within the total population of accounts that did default. . The diagonal . The red dashed line dividing the plot represents random performance. When the FPR matches the TPR, we might as well be guessing–you’re right as often as you’re wrong. . If your curve falls to the left of the diagonal, the model is performing better than random because your true positive rate exceeds your false positive rate. Likewise, a curve to the right of the diagonal indicates some systemic error in your model, causing it to perform worse than random. . The curve . Now that we’ve established the space in which we’re working, we can best understand how to determine the ROC curve above. . The ROC visualizes the trade-offs between the FPR and the TPR when we adjust the threshold by which the classifier makes its determination. . For example, our model could determine a credit default using a probability threshold of 0.25. If the model’s probability of default is 0.32, we would return “default”. If the probability is 0.19, we could return “no default”. . . This is illustrated in the ROC plot above. The FPR and TPR for a threshold of 0.25 is represented by the black dot on the ROC curve. Our TPR is roughly 0.7 and our FPR is around 0.45. . Now, let’s lower the threshold to 0.15. . . We move further up the curve, increasing our TPR to nearly 0.9 but also increasing our FPR to about 0.75. . This illustrates the principle that as we decrease our decision threshold, we move to the right and upwards along the curve because we are simply classifying more observations overall as positives. Therefore, TPR increases as we capture more of those true positives but at the same time, our probability of false alarm also goes up as we become less strict with our requirement to classify a positive. . Choosing a threshold . This observation begs the question–what would be the optimal threshold to choose? . Of course, this will likely depend on model-specific considerations. For example, what is the cost of a false positive? If it’s relatively low, we might as well increase our TPR at the expense of an increased FPR as well. . However, if we’d like to try and balance these two competing metrics, we can choose the point along the curve that is closest to the top-left corner of the plot. This could be considered the apex of the curve, as shown below. . . The apex can be found by determining where on the curve we find a maximum difference between TPR and FPR. In our case, this occurs at a threshold of 0.29 to return a TPR of nearly 0.6 and a FPR of about 0.36. . AUC . Perhaps the most widely used application of an ROC curve is to calculate the area underneath it. This is called AUC or “Area Under Curve”. . The area in gray below represents the AUC. . . Our AUC for this model is 0.65. A random model would produce an AUC of 0.5 so we are doing better than guessing! . An ideal model would have an ROC curve that hugs the axes with an apex close to the upper-left corner, resulting in an AUC of nearly 1.0. Therefore, AUC is a way to quantify the ability of the model to maximize TPR while minimizing FPR, no matter the threshold chosen. .",
            "url": "https://mdhanna.github.io/blog/data%20science%20basics/2020/06/10/what-is-an-roc-curve.html",
            "relUrl": "/data%20science%20basics/2020/06/10/what-is-an-roc-curve.html",
            "date": " • Jun 10, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Data Science 101: Precision vs. recall",
            "content": "Congrats! You’ve built a binary classifier that you’re convinced is totally awesome. . But how do you quantify just how awesome this model is? And more specifically, how do you communicate this model’s level of awesomeness to your manager/product owner/stakeholder/random-person-on-the-street? . This is where performance metrics such as precision and recall come into play, and we’ll attempt to explain the intuition around these in addition to their definitions. . What is the positive class? . If you’ve built a binary classifier, perhaps the first step in determining your performance metric is to select a “positive” class. This is easy in some instances (ex. coronavirus test result) and less so in others (ex. determining if your pet is either a cat or a chinchilla). But establishing these definitions early (and stating them explicitly) will save a lot of confusion down the road. . False negatives vs. true positives . Something I struggled with initially was what exactly did we mean by a “false negative”? Was it a true negative that we classified incorrectly? Or did the model return an incorrect (and therefore false) prediction of negative? In other words, from whose perspective do we consider this classification false? . The answer is the latter definition above. The table below sums this up. . . Confusion matrix . Once you’ve established your false positives and false negatives, you can display them in a confusion matrix that looks very similar to the table above. . Here is an example for a classifier that attempts to determine if shoplifting is taking place (where we define a shoplifting incident as a positive). Let’s say for example, this model takes video footage from a store surveillance system and looks for certain features (ex. a customer picking up an item and hiding it under his/her shirt) that would indicate shoplifting. . The confusion matrix shows the number of observations for each class and the corresponding predictions from the model. . From the matrix above, we can see that our classifier is rather paranoid and often mistakes normal behavior for shoplifting. . Accuracy . Now we can start to sum up the classifier’s performance using a single value, such as the accuracy, which represents the fraction of correct predictions out of the total. In the shoplifting example, the accuracy is shown by the following: . . $accuracy = frac{ text{ true positives} + text{ true negatives}}{ text{ total classified}} = frac{40+ 107}{40+ 107+ 345 + 8} = 0.294$ . Unfortunately, this model is performing far worse than random. . WARNING: Note that accuracy is a misleading metric in this case due to unbalanced class sizes. In other words, because we have so few true shoplifting incidences compared to cases of normal behavior, we can easily achieve an accuracy of 0.904 by returning a prediction of “normal” every time. But no one would consider such a classifier to be truly “accurate”. . Precision and Recall . I mentioned earlier that our classifier tends to overpredict shoplifting–how can we incorporate this tendency into a performance metric? . This is where precision and recall come into play. These metrics are class-specific, which means that we must specify a value for both precision and recall for each class returned by the model. . Precision . Precision is the answer to the question: out of the total predictions for a certain class returned by the model, how many were actually correct? For example, the precision for the shoplifting class is: . . $precision = frac{TP}{TP + FP}$ . . $precision_{shoplifting} = frac{ text{ true shoplifting}}{ text{ total predicted shoplifting}} = frac{40}{40+345} = 0.104$ . . Similarly, for the normal behavior: . . $precision_{normal} = frac{ text{ true normal}}{ text{ total predicted normal}} = frac{107}{107+8} = 0.930$ . . In other words, the model’s predictions for normal behavior were correct 93% of the time, while its predictions for shoplifting were correct only 10% of the time. Yikes. . Recall . I like to think of recall as a class-specific accuracy. How many of the model’s predictions for a certain class were actually correct? . $recall = frac{TP}{TP + FN}$ . . $recall_{shoplifting} = frac{ text{ true shoplifting}}{ text{ total actual shoplifting}} = frac{40}{40+8} = 0.833$ . . $recall_{normal} = frac{ text{ true normal}}{ text{ total actual normal}} = frac{107}{107+345} = 0.237$ . . The model correctly identified 83% of the actual shoplifting incidents. . And here we see the trade-off often inherent in precision and recall. The model correctly predicted a good majority (83%) of the actual shoplifting incidents, but at the expense of also erroneously predicting many truly normal behaviors as shoplifting too (nearly 90% of the predicted shoplifting incidents). . This ties into Type I and Type II errors, where a type I error is a false positive (normal behavior incorrectly classified as shoplifting) and a type II error is a false negative (shoplifting incorrectly classified as normal behavior). . For our situation, this boils down to asking if you would rather have the police called on an innocent customer (type I) or lose merchandise to unchecked shoplifters (type II)? Understanding the costs of type I and type II errors helps to weigh whether you’d like to improve either recall or precision. Often, it’s difficult to do both simultaneously. . F1-Score . However, if you’d like to tie up both precision and recall into one single metric to hand over to your manager/product owner/stakeholder, then the F1-score (sometimes called F-measure) is for you! . This is simply the harmonic mean of the precision and recall for a given class, shown below. . . $F1 = 2 * frac{precision * recall}{precision + recall}$ . . An F1-score of 1 indicates perfect precision and recall. . If you’d like to place more importance on recall over precision, you can introduce a $ beta$ term (set $ beta$ to a value less than 1 to place more emphasis of precision instead of recall). . . $F_{ beta} = (1 + beta^2) * frac{precision * recall}{( beta^2 * precision) + recall}$ . .",
            "url": "https://mdhanna.github.io/blog/data%20science%20basics/2020/05/31/precision-vs-recall.html",
            "relUrl": "/data%20science%20basics/2020/05/31/precision-vs-recall.html",
            "date": " • May 31, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://mdhanna.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mdhanna.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}